---
title: "Multiple Linear Regression"
subtitle: "Special Predictors"
author: "Dr. Maria Tackett"
date: "02.06.19"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "sta210-slides.css"
    logo: img/sta210-sticker-icon.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE, echo=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
	fig.align = "center",
	fig.height = 4,
	message = FALSE,
	warning = FALSE, 
	dpi = 300
)
```



## Announcements

- Lab 03 due today 

- HW 02 due Mon, Feb 11 

- Policy on [regrade requests](https://www2.stat.duke.edu/courses/Spring19/sta210.001/policies.html)

---

 
## R packages

```{r}
library(tidyverse)
library(knitr)
library(broom)
library(Sleuth3) # case 1202 dataset
library(cowplot) # use plot_grid function
```

---

## Starting wages data

**Explanatory**
- <font class="vocab">`Educ`: </font>years of Education
- <font class="vocab">`Exper`: </font>months of previous work Experience (before hire at bank)
- <font class="vocab">`Female`: </font>1 if female, 0 if male
- <font class="vocab">`Senior`: </font>months worked at bank since hire
- <font class="vocab">`Age`: </font>Age in months

**Response**
- <font class="vocab">`Bsal`: </font>annual salary at time of hire

---

## Starting wages 

```{r}
wages <- case1202 %>% 
  mutate(Female = ifelse(Sex=="Female",1,0)) %>%
  select(-Sal77,-Sex)
```

```{r}
glimpse(wages)
```

---


## Regression model

```{r}
model <- lm(Bsal ~ Senior + Age + Educ + Exper + Female, 
            data=wages)
kable(tidy(model,conf.int=TRUE),format="html",digits=3)
```

---

class: middle, center

## Math Details 


---

## Regression Model 

- The multiple linear regression model assumes 

$$\color{blue}{y|x_1, x_2,  \ldots, x_p \sim N(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p, \sigma^2)}$$
<br> 

--

- For a given observation $(x_{i1}, x_{i2}, \ldots, x_{iP}, y_i)$, we can rewrite the previous statement as 

$$\color{blue}{y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip} + \epsilon_{i} \hspace{10mm} \epsilon_i \sim N(0,\sigma^2)}$$

---

## Estimating $\sigma^2$

- For a given observation $(x_{i1}, x_{i2}, \ldots,x_{ip}, y_i)$ the residual is 
$$\color{blue}{e_i = y_{i} - (\hat{\beta}_0 + \hat{\beta}_1 x_{i1} + \hat{\beta}_{2} x_{i2} + \dots + \hat{\beta}_p x_{ip})}$$

--

- The estimated value of the regression variance , $\sigma^2$, is 

$$\color{blue}{\hat{\sigma}^2  = \frac{RSS}{n-p-1} = \frac{\sum_{i=1}^ne_i^2}{n-p-1}}$$

---

# Estimating Coefficients 

- One way to estimate the coefficients is by taking partial derivatives of the formula

$$\color{blue}{\sum_{i=1}^n e_i^2 = \sum_{i=1}^{n}[y_i - (\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} \dots + \beta_p x_{ip})]^2}$$

--

- This produces messy formulas, but we can represent the regression equation using matrix notation

---


## Matrix Notation 
 
- $\mathbf{Y}$: $n \times 1$ vector of responses

- $\mathbf{X}$: $n \times (p+1)$ matrix of intercept and predictor variables

- $\boldsymbol{\beta}$: $(p+1) \times 1$ vector of coefficients

$$\mathbf{Y} = \begin{bmatrix}
    y_{1} \\
    y_{2} \\
    \vdots\\
     y_{n}
\end{bmatrix} \hspace{15mm} \mathbf{X} = \begin{bmatrix}
1& x_{11} & x_{12} & \dots & x_{1p}\\
1&x_{21} & x_{22} & \dots & x_{2p}\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1&x_{n1} & x_{n2} & \dots & x_{np} \end{bmatrix}\hspace{15mm}\boldsymbol{\beta}=\begin{bmatrix}
    \beta_0 \\
    \beta_1 \\
    \vdots\\
     \beta_p
\end{bmatrix}$$
<br> 

--

.alert[
$$\mathbf{Y} = \mathbf{X}\boldsymbol{\beta}$$
]

---

## Estimate $\beta_j$'s

.alert[
$$\hat{\boldsymbol{\beta}} = (\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$$
]

```{r}
# Estimate coefficients
y <- wages %>% select(Bsal) %>% as.matrix()
predictors <- wages %>% select(Senior, Age, Educ, Exper, Female) 
intercept <- data.frame(intercept=rep(1,nrow(wages)))
x <- bind_cols(intercept, predictors) %>% as.matrix()
beta <-  solve(t(x)%*%x)%*%t(x)%*%y
beta
```

---

## Estimate $\sigma^2$

.alert[
$$\hat{\sigma}^2 ~ = ~ \frac{\mathbf{e}^{T}\mathbf{e}}{n-p-1} ~ = ~  \frac{(\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}})^{T}(\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}})}{n-p-1}$$
]


```{r}
# Estimate sigma.sq
e <-  y - x%*%beta #residuals
sigma.sq <- (t(e)%*%e)/(nrow(wages)-ncol(x))
(sigma.sq <- as.numeric(sigma.sq))
```

---

## Calculate $SE(\hat{\beta}_j)$

.alert[
$$Var(\boldsymbol{\hat{\beta}}) = (\mathbf{X}^T\mathbf{X})^{-1}\hat{\sigma}^2$$
]

Take the square root of the diagonal elements to get $SE(\hat{\beta}_j)$

```{r}
var.beta = solve(t(x)%*%x)*sigma.sq
SE = sqrt(diag(var.beta))
SE
```

---

class: middle

See [https://github.com/STA210-Sp19/supplemental-notes/blob/master/regression-basics-matrix.pdf](https://github.com/STA210-Sp19/supplemental-notes/blob/master/regression-basics-matrix.pdf) for more details about the matrix form of linear regressin. 

---

class: middle, center

## Special Predictors

---

## Interpreting the Intercept 

```{r echo=FALSE}
kable(tidy(model),format="html",digits=3)
```

.question[
- Interpret the intercept. 

- Is this interpretation meaningful? Why or why not?
]
---



### Mean-Centered Variables

- To aid with the interpretation of the intercept, you can subtract each continuous predictor variable from its mean value


- Use these <font class="vocab3">mean-centered</font> variables in the regression model 


- Now the intercept is interpreted as the expected (i.e. mean) value of the response at the average value of the predictor variables

---

 

### Salary: Mean-Centered Variables

```{r}
wages <- wages %>%
  mutate(SeniorCent = Senior - mean(Senior), 
         AgeCent = Age-mean(Age), 
         EducCent = Educ - mean(Educ), 
         ExperCent = Exper - mean(Exper))
```

.pull-left[
```{r,echo=F}
ggplot(data=wages,aes(x=Senior)) + geom_histogram(fill="steelblue2",color="black") + 
labs(title="Seniority",x="Seniority")
```
]
.pull-right[
```{r,echo=F}
ggplot(data=wages,aes(x=SeniorCent)) + geom_histogram(fill="green",color="black") + 
labs(title="Mean-Centered Seniority",xlab="Mean-Centered Seniority")
```
]


---

 

### Salary: Mean-Centered Variables

```{r, include=FALSE}
modelCent <- lm(Bsal ~ Female + SeniorCent + AgeCent + 
                    EducCent + ExperCent, data=wages)
kable(tidy(modelCent,conf.int=TRUE),format="html",digits=3)
wages <- wages %>% mutate(residuals=resid(modelCent))
```

.question[
Calculate the regression model using the mean-centered variables. How did the model change?
]

---

## Quadratic Terms

 - Sometimes the response variable may have a quadratic relationship with one or more predictor variables
    - You can see this in a plot of the residuals vs. a predictor variable 
- Include quadratic terms in the model to capture the relationship 

- <font class="vocab">Good Practice:</font> Also include all lower order terms 
This helps with interpretation 
  
- You can show quadratic relationships by plotting the predicted mean response for different values of the explanatory variable

- The same applies for higher order polynomial relationships 

---

Below are plots of the residuals versus each quantitative predictor variable. 

```{r,echo=F}
p1 <- ggplot(data=wages,aes(x=Senior,y=residuals)) + 
  geom_point() + 
  geom_hline(yintercept=0,color="red") +
  labs(x = "Seniority", y="Residuals")

p2 <- ggplot(data=wages,aes(x=Age,y=residuals)) + 
  geom_point() + 
  geom_hline(yintercept=0,color="red") +
  labs(x = "Age", y="Residuals")

p3 <- ggplot(data=wages,aes(x=Educ,y=residuals)) + 
  geom_point() + 
  geom_hline(yintercept=0,color="red") +
  labs(x = "Education", y="Residuals")

p4 <- ggplot(data=wages,aes(x=Exper,y=residuals)) + 
  geom_point() + 
  geom_hline(yintercept=0,color="red") +
  labs(x = "Experience", y="Residuals")

plot_grid(p1,p2,p3,p4,nrow=2)
```

.question[
Which variables (if any) appear to have a quadratic relationship with `Bsal`?
]

---

## Indicator (dummy) variables


- Suppose there is a categorical variable with $k$ levels (categories)

- Make $k$ indicator (dummy) variables

- Use $k-1$ of the indicator variables in the model
    - Can't uniquely estimate all $k$ variables at once if the intercept is in the model
    
- Level that doesn't have a variable in the model is called the <font class="vocab3">baseline</font>

- Coefficients interpreted as the change in the mean of the response over the baseline

---

## Indicator variables when $k=2$

```{r echo=FALSE}
kable(tidy(modelCent,conf.int=TRUE),format="html",digits=3)
```

.question[
- What is the intercept of the model for males? 

- What is the intercept of the model for females?
]

---

## Indicator variables when $k > 2$

Build a regression model with Education treated as a categorical variable. 

.question[
- What is the baseline for Education?

- Interpret the coefficient for `EducCat16`.

- What is your conclusion from the p-value of `EducCat12`?

- What is your conclusion from the p-value of `EducCat15`?
]
 
 
---

## Interaction Terms

- **Case**: Relationship of the predictor variable with the response depends on the value of another predictor variable
  + This is an <font class="vocab3">interaction effect</font>
  
- Create a new interaction variable that is one predictor variable times the other in the interaction 

- <font class="vocab">Good Practice:</font> When including an interaction term, also include each variable on its own (called **main effect**)

---

## Interaction effects

```{r,echo=F}
ggplot(data=wages, aes(x=Senior,y=Bsal,color=as.factor(Female))) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE) + 
  labs(title = "Starting Salary vs. Seniority by Gender", 
       x = "Months at Company", 
       y = "Starting Salary")
```

.question[
Do you think there is a significant interaction effect that includes gender and seniority? Why or why not?
]


---

## Before next class

- [Reading 05](https://www2.stat.duke.edu/courses/Spring19/sta210.001/reading/reading-05.html)