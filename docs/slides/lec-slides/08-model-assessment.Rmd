---
title: "Transformations & Model Assessment"
author: "Dr. Maria Tackett"
date: "02.11.19"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "sta210-slides.css"
    logo: img/sta210-sticker-icon.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE, echo=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
	fig.align = "center",
	fig.height = 4,
	message = FALSE,
	warning = FALSE, 
	dpi = 300
)
```


## Announcements

- HW 02 due today

- Lab 04 due Wednesday 

- HW 03 due Monday, Feb 18

---

 
## R packages

```{r}
library(tidyverse)
library(knitr)
library(broom)
library(cowplot) # use plot_grid function
library(Sleuth3) #ex0824 data
```

---

## Respiratory Rate vs. Age

- A high respiratory rate can potentially indicate a respiratory infection in children. In order to determine what indicates a "high" rate, we first want to understand the relationship between a child's age and their respiratory rate. 

- The data contain the respiratory rate for 618 children ages 15 days to 3 years.

- **Variables**: 
    - <font class="vocab">`Age`</font>: age in months
    - <font class="vocab">`Rate`</font>: respiratory rate (breaths per minute)

---

## Rate vs. Age

```{r}
respiratory <- ex0824
ggplot(data=respiratory, aes(x=Age, y=Rate)) +
  geom_point() + 
  labs("Respiratory Rate vs. Age")
```

---

## Rate vs. Age

```{r echo=FALSE}
model <- lm(Rate ~ Age, data=respiratory)
kable(tidy(model, conf.int=TRUE),format="html", digits=3)
```

```{r echo=FALSE}
respiratory <- respiratory %>% 
  mutate(residuals = resid(model), predicted = predict(model))

ggplot(data=respiratory, aes(x=predicted, y=residuals)) +
  geom_point() + 
  geom_hline(yintercept=0,color="red")
```

---

class: middle, center
## Log transformations

---

## Need to transform $y$ 

- Typically, a "fan-shaped" residual plot indicates the need for a transformation of the response variable $y$
  + $\mathbf{\color{green}{\log(y)}}$: Easiest to interpret 

--


- When building a model: 
  + Choose a transformation and build the model on the transformed data
  + Reassess the residual plots
  + If the residuals plots did not sufficiently improve, try a new transformation!

---

## Log transformation on $y$

- Use when the residual plot shows "fan-shaped" pattern

- If we apply a log transformation to the response variable, we want to estimate the parameters for the model...
.alert[
$$\log(y) = \beta_0 + \beta_1 x$$
]

--

- We want to interpret the model in terms of $y$ not $\log(y)$, so we write all interpretations in terms of 

.alert[
$$y = \exp\{\beta_0 + \beta_1 x\} = \exp\{\beta_0\}\exp\{\beta_1x\}$$
]
---

### Mean and median of $\log(y)$

- Recall that $y = \beta_0 + \beta_1 x_i$ is the **mean** value of $y$ at the given value $x_i$. This doesn't hold when we log-transform $y$

--

- The mean of the logged values is **not** equal to the log of the mean value. Therefore at a given value of $x$

$$\color{blue}{\begin{aligned}\exp\{\text{Mean}(\log(y))\} \neq \text{Mean}(y) \\[5pt]
\Rightarrow \exp\{\beta_0 + \beta_1 x\} \neq \text{Mean}(y) \end{aligned}}$$

--

- However, the median of the logged values **is** equal to the log of the median value. Therefore, 
$$\color{blue}{\exp\{\text{Median}(\log(y))\} = \text{Median}(y)}$$

--

- If the distribution of $\log(y)$ is symmetric about the regression line, for a given value $x_i$,
$$\color{blue}{\text{Median}(\log(y)) = \text{Mean}(\log(y))}$$

---

### Interpretation with log-transformed $y$

- Given the previous facts, if $\log(y) = \beta_0 + \beta_1 x$, then 
.alert[
$$\text{Median}(y) = \exp\{\beta_0\}\exp\{\beta_1x\}$$
]
<br>

--

- <font class="vocab">Intercept:</font> When $x=0$, the median of $y$ is expected to be $\exp\{\beta_0\}$

--

- <font class="vocab">Slope: </font>For every one unit increase in $x$, the median of $y$ is expected to multiply by a factor of $\exp\{\beta_1\}$


---

## log(Rate) vs. Age

```{r}
respiratory <- respiratory %>% mutate(log_rate = log(Rate))
```

```{r echo=F}
ggplot(data=respiratory, aes(x=Age,y=log_rate)) + 
  geom_point() +
  labs(title="Log-Transformed Rate vs. Age")
```

---

## log(Rate) vs. Age

```{r}
log_model <- lm(log_rate ~ Age, data = respiratory)
```

```{r echo=F}
ggplot(data=respiratory, aes(x=predict(log_model), y=resid(log_model))) +
  geom_point() + 
  geom_hline(yintercept=0, color="red") +
  labs(x="Predicted", y="Residuals",
       title="Residuals vs. Predicted", 
       subtitle="log-transformed Rate")
```

---

## log(Rate) vs. Age

```{r}
kable(tidy(log_model, conf.int=TRUE),format="html", digits=3)
```
<br>

.question[
1. Write the model in terms of $\log(rate)$. 
2. Write the model in terms of $rate$. Interpret the slope and intercept.

]

---

## Confidence interval for $\beta_j$

- The confidence interval for the coefficient of $x$ describing its relationship with $\log(y)$ is

$$\hat{\beta}_1 \pm t^* SE(\hat{\beta_j})$$

--

- The confidence interval for the coefficient of $x$ describing its relationship with $y$ is

.alert[
$$\exp\big\{\hat{\beta}_1 \pm t^* SE(\hat{\beta_j})\big\}$$
]

---

## Coefficient of `Age`

```{r }
kable(tidy(log_model, conf.int=TRUE),format="html", digits=3)
```

.question[
Interpret the 95% confidence interval for the coefficient of `Age` in terms of `Rate`.
]

---

## Log Transformation on $x$

.pull-left[
```{r,echo=F}
set.seed(1)
s <- ggplot2::diamonds %>% sample_n(100)
ggplot(data=s,aes(x=carat,y=log(price)))+
  geom_point(color="blue")+
  ggtitle("Scatterplot")+
  xlab("X")+
  ylab("Y") + 
  theme(plot.title = element_text(hjust = 0.5,size=20))
```
]

.pull-right[
```{r,echo=F}
mod2 <- lm(log(price) ~ carat, data=s)
s <- s %>% mutate(residuals = resid(mod2))
ggplot(data=s,aes(x=carat,y=residuals)) + 
geom_point()+
geom_hline(yintercept=0,color="red") +
  ggtitle("Residual Plot")+
  xlab("X")+
  ylab("residuals") + 
  theme(plot.title = element_text(hjust = 0.5,size=20))
```
]

- Try a transformation on $X$ if the scatterplot shows some curvature but the variance is constant for all values of $X$

---

## Model with Transformation on $x$

.alert[
$$y = \beta_0 + \beta_1 \log(x)$$
]
<br> 

--

- <font class="vocab">Intercept: </font> When $\log(x)=0$, $(x=1)$, $y$ is expected to be $\beta_0$ (i.e. the mean of $y$ is $\beta_0$)

--

- <font class="vocab">Slope: </font> When $x$ is multiplied by a factor of $\mathbf{C}$, $y$ is expected to change by $\boldsymbol{\beta_1}\mathbf{\log(C)}$ units, i.e. the mean of $y$ changes by $\boldsymbol{\beta_1}\mathbf{\log(C)}$
    - *Example*: when $x$ is multiplied by a factor of 2, $y$ is expected to change by $\boldsymbol{\beta_1}\mathbf{\log(2)}$ units

---

## Rate vs. log(Age)

```{r,echo=F}
ex0824 <- ex0824 %>% mutate(log.age = log(Age))
ggplot(data=ex0824,aes(x=log.age,y=Rate)) + 
  geom_point()  +
  ggtitle("Respiratory Rate vs. log(Age)") + 
  xlab("log(Age)")+
  ylab("Respiratory Rate")
```

---

## Respiratory Rate vs. Age

```{r,echo=F}
mod3 <- lm(Rate ~ log.age,data=ex0824)
kable(tidy(mod3, conf.int=TRUE),format="html")
```
<br> 

.question[

1. Write the equation for the model of $y$ regressed on $\log(x)$. 

2. Interpret the intercept in the context of the problem. 

3. Interpret the slope in terms of how the mean respiratory rate changes when a child's age doubles. 

4. Suppose a doctor has a patient who is currently 3 years old. Will this model provide a reliable prediction of the child's respiratory rate when her age doubles? Why or why not?
]
---

class: middle, center

## Model Assessment

---

## ANOVA table for regression

We can use the Analysis of Variance (ANOVA) table to decompose the variability in our response variable


|  | Sum of Squares | DF | Mean Square | F-Stat| p-value |
|------------------|----------------|--------------------|-------------|-------------|--------------------|
| Regression (Model) | $$\sum\limits_{i=1}^{n}(\hat{y}_i - \bar{y})^2$$ | $$p$$ | $$\frac{MSS}{p}$$ | $$\frac{MMS}{RMS}$$ | $$P(F > \text{F-Stat})$$ |
| Residual | $$\sum\limits_{i=1}^{n}(y_i - \hat{y}_i)^2$$ | $$n-p-1$$ | $$\frac{RSS}{n-p-1}$$ |  |  |
| Total | $$\sum\limits_{i=1}^{n}(y_i - \bar{y})^2$$ | $$n-1$$ | $$\frac{TSS}{n-1}$$ |  |  |


The estimate of the regression variance, $\hat{\sigma}^2 = MSE$

---

## $R^2$ 

- **Recall**: $R^2$ is the proportion of the variation in the response variable explained by the regression model
<br>

--

- $R^2$ will always increase as we add more variables to the model 
  + If we add enough variables, we can always achieve $R^2=100\%$
<br>

--

- If we only use $R^2$ to choose a best fit model, we will be prone to choose the model with the most predictor variables


---

## Adjusted $R^2$

- <font class="vocab">Adjusted $R^2$</font>: a version of $R^2$ that penalizes for unnecessary predictor variables
<br> 

- Similar to $R^2$, it measures the proportion of variation in the response that is explained by the regression model 
<br>

- Differs from $R^2$ by using the mean squares rather than sums of squares and therefore adjusting for the number of predictor variables

---

## $R^2$ and Adjusted $R^2$

$$R^2 = \frac{\text{Total Sum of Squares} - \text{Residual Sum of Squares}}{\text{Total Sum of Squares}}$$
<br>

--

.alert[
$$Adj. R^2 = \frac{\text{Total Mean Square} - \text{Residual Mean Square}}{\text{Total Mean Square}}$$
]
<br>

--

- $Adj. R^2$ can be used as a quick assessment to compare the fit of multiple models; however, it should not be the only assessment!

--

- Use $R^2$ when describing the relationship between the response and predictor variables

---

## Restaurant tips

What affects the amount of customers tip at a restaurant? 

- **Response:**
    - <font class="vocab">`Tip`</font>: amount of the tip
    
- **Predictors:**
    - <font class="vocab">`Party`</font>: number of people in the party
    - <font class="vocab'>`Meal`</font>:  time of day (Lunch, Dinner, Late Night) 
    - <font class="vocab">`Age`</font>: age category of person paying the bill (Yadult, Middle, SenCit)

```{r}
tips <- read_csv("data/tip-data.csv") %>%
  filter(!is.na(Party))
```


---

## Restaurant tips ANOVA

```{r}
model1 <- aov(Tip ~ Party + Meal + Age , data = tips)
kable(tidy(model1),format="html",digits=3)
```

---

```{r echo=F}
kable(tidy(model1),format="html",digits=3)
```

```{r}
#r-squared
tss <- 1188.63588 + 88.46005 + 13.03236 + 622.97932	
rss <- 622.97932	
(r_sq <- (tss - rss)/tss)
```

```{r}
#adj r-squared
tms <- tss/(nrow(tips)-1)
rms <- 3.821959	
(adj_r_sq <- (tms - rms)/tms)
```

---

### Restaurant tips: $R^2$ and Adj. $R^2$

```{r}
glance(model1)
```
<br>

- Close values of $R^2$ and Adjusted $R^2$ indicate that the variables in the model are significant in understanding variation in tips

---

## ANOVA F Test

- Using the ANOVA table, we can test whether any variable in the model is a significant predictor of the response. We conduct this test using the following hypotheses: 

.alert[
$$\begin{aligned}&H_0: \beta_{1} =  \beta_{2} = \dots = \beta_p = 0 \\ 
&H_a: \text{at least one }\beta_j \text{ is not equal to 0}\end{aligned}$$
]

<br>

- The statistic for this test is the $F$ test statistic in the ANOVA table 

- We calculate the p-value using an $F$ distribution with $p$ and $(n-p-1)$ degrees of freedom

---

## ANOVA F Test in R

```{r}
model0 <- lm(Tip ~ 1, data=tips)
```

--

```{r}
model1 <- aov(Tip ~ Party + Meal + Age , data = tips)
```

--

```{r}
kable(anova(model0,model1),format="html")
```

---

### Testing subset of coefficients

- Sometimes we want to test whether a subset of coefficients are all equal to 0
    - This is often the case when we want test whether a categorical variable with $k$ levels is a significant predictor of the response

- To do so, we will use the  <font class="vocab3">Nested F Test </font> 

---

## Nested F Test

- Suppose we have a full and reduced model: 

$$\begin{aligned}&\text{Full}: y = \beta_0 + \beta_1 x_1 + \dots + \beta_q x_q + \beta_{q+1} x_{q+1} + \dots \beta_p x_p \\
&\text{Red}: y = \beta_0 + \beta_1 x_1 + \dots + \beta_q x_q\end{aligned}$$
<br>

- We want to test whether any of the variables $\beta_{q+1} x_{q+1}, \ldots, \beta_p x_p$ are significant predictors. To do so, we will test the hypothesis: 

.alert[
$$\begin{aligned}&H_0: \beta_{q+1} =  \beta_{q+2} = \dots = \beta_p = 0 \\ 
&H_a: \text{at least one }\beta_j \text{ is not equal to 0}\end{aligned}$$
]

---

## Nested F Test

- The test statistic for this test is 


$$F = \frac{(RSS_{reduced} - RSS_{full})\big/(p_{full} - p_{reduced})}{RSS_{full}\big/(n-p_{full}-1)}$$
<br> 

- Calculate the p-value using the F distribution with $(p_{full} - p_{reduced})$ and $(n-p_{full}-1)$ degrees of freedom

---

### Is `Meal` a significant preddictor of tips?

```{r echo=F}
model.tips <- lm(Tip ~ Party + Age + Meal,data=tips)
kable(tidy(model.tips),format="html", digits=3)
```

.question[
Why is it not good practice to use the individual p-values to determine whether `Meal` (or any categorical variable) is significant?
]

---

### Tips data:  Nested F Test

$$\begin{aligned}&H_0: \beta_{late night} = \beta_{lunch} = 0\\
&H_a: \text{ at least one }\beta_j \text{ is not equal to 0}\end{aligned}$$

--

```{r}
reduced <- lm(Tip ~ Party + Age, data = tips)
```

--

```{r}
full <- lm(Tip ~ Party + Age + Meal, data = tips)
```

--

```{r}
kable(anova(full,reduced),format="html")
```

--

**At least one coefficient associated with `Meal` is not zero. Therefore, `Meal` is a significant predictor of `Tips`.**



