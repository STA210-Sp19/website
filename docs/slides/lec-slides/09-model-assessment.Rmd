---
title: "Model Assessment"
author: "Dr. Maria Tackett"
date: "02.13.19"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "sta210-slides.css"
    logo: img/sta210-sticker-icon.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE, echo=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
	fig.align = "center",
	fig.height = 4,
	message = FALSE,
	warning = FALSE, 
	dpi = 300
)
```


## Announcements

- Lab 04 due today

- HW 03 due Monday, Feb 18

---

## R packages

```{r}
library(tidyverse)
library(knitr)
library(broom)
library(cowplot) # use plot_grid function
library(Sleuth3) #ex0824 data
```

---

## Restaurant tips

What affects the amount of customers tip at a restaurant? 

- **Response:**
    - <font class="vocab">`Tip`</font>: amount of the tip
    
- **Predictors:**
    - <font class="vocab">`Party`</font>: number of people in the party
    - <font class="vocab">`Meal`</font>:  time of day (Lunch, Dinner, Late Night) 
    - <font class="vocab">`Age`</font>: age category of person paying the bill (Yadult, Middle, SenCit)

```{r}
tips <- read_csv("data/tip-data.csv") %>%
  filter(!is.na(Party))
```


---

## ANOVA table for regression

We can use the Analysis of Variance (ANOVA) table to decompose the variability in our response variable


|  | Sum of Squares | DF | Mean Square | F-Stat| p-value |
|------------------|----------------|--------------------|-------------|-------------|--------------------|
| Regression (Model) | $$\sum\limits_{i=1}^{n}(\hat{y}_i - \bar{y})^2$$ | $$p$$ | $$\frac{MSS}{p}$$ | $$\frac{MMS}{RMS}$$ | $$P(F > \text{F-Stat})$$ |
| Residual | $$\sum\limits_{i=1}^{n}(y_i - \hat{y}_i)^2$$ | $$n-p-1$$ | $$\frac{RSS}{n-p-1}$$ |  |  |
| Total | $$\sum\limits_{i=1}^{n}(y_i - \bar{y})^2$$ | $$n-1$$ | $$\frac{TSS}{n-1}$$ |  |  |


The estimate of the regression variance, $\hat{\sigma}^2 = RMS$

---

## $R^2$ 

- **Recall**: $R^2$ is the proportion of the variation in the response variable explained by the regression model
<br>

--

- $R^2$ will always increase as we add more variables to the model 
  + If we add enough variables, we can always achieve $R^2=100\%$
<br>

--

- If we only use $R^2$ to choose a best fit model, we will be prone to choose the model with the most predictor variables


---

## Adjusted $R^2$

- <font class="vocab">Adjusted $R^2$</font>: a version of $R^2$ that penalizes for unnecessary predictor variables
<br> 

- Similar to $R^2$, it measures the proportion of variation in the response that is explained by the regression model 
<br>

- Differs from $R^2$ by using the mean squares rather than sums of squares and therefore adjusting for the number of predictor variables

---

## $R^2$ and Adjusted $R^2$

$$R^2 = \frac{\text{Total Sum of Squares} - \text{Residual Sum of Squares}}{\text{Total Sum of Squares}}$$
<br>

--

.alert[
$$Adj. R^2 = \frac{\text{Total Mean Square} - \text{Residual Mean Square}}{\text{Total Mean Square}}$$
]
<br>

--

- $Adj. R^2$ can be used as a quick assessment to compare the fit of multiple models; however, it should not be the only assessment!

--

- Use $R^2$ when describing the relationship between the response and predictor variables

---

### Restaurant tips: model

```{r}
model1 <- lm(Tip ~ Party + Meal + Age , data = tips)
kable(tidy(model1),format="html",digits=3)
```

---

## Restaurant tips: ANOVA

- <font class="vocab">R output</font>

```{r echo=F}
kable(anova(model1),format="html",digits=3)
```

--

- <font class="vocab">ANOVA table</font>

|  | Sum of Squares | DF | Mean Square | F-Stat| p-value |
|------------------|----------------|--------------------|-------------|-------------|--------------------|
| Regression (Model) | `r 1188.63588 + 88.46005 + 13.03236` | 5 | `r (1188.63588 + 88.46005 + 13.03236)/5` | `r ((1188.63588 + 88.46005 + 13.03236)/5)/(622.97932/163)` | 0 |
| Residual | 622.97932	 | 163 | `r 622.97932/163`  |  |  |
| Total | `r 1188.63588 + 88.46005 + 13.03236 + 622.97932	`  | 168 |  |  |  |

---

### Calculating $R^2$ and Adj $R^2$

|  | Sum of Squares | DF | Mean Square | F-Stat| p-value |
|------------------|----------------|--------------------|-------------|-------------|--------------------|
| Regression (Model) | `r 1188.63588 + 88.46005 + 13.03236` | 5 | `r (1188.63588 + 88.46005 + 13.03236)/5` | `r ((1188.63588 + 88.46005 + 13.03236)/5)/(622.97932/163)` | 0 |
| Residual | 622.97932	 | 163 | `r 622.97932/163`  |  |  |
| Total | `r 1188.63588 + 88.46005 + 13.03236 + 622.97932	`  | 168 |  |  |  |


```{r}
#r-squared
tss <- 1188.63588 + 88.46005 + 13.03236 + 622.97932	
rss <- 622.97932	
(r_sq <- (tss - rss)/tss)
```

```{r}
#adj r-squared
tms <- tss/(nrow(tips)-1)
rms <- 3.821959	
(adj_r_sq <- (tms - rms)/tms)
```

---

### Restaurant tips: $R^2$ and Adj. $R^2$

```{r}
glance(model1)
```
<br>

- Close values of $R^2$ and Adjusted $R^2$ indicate that the variables in the model are significant in understanding variation in tips

---

## ANOVA F Test

- Using the ANOVA table, we can test whether any variable in the model is a significant predictor of the response. We conduct this test using the following hypotheses: 

.alert[
$$\begin{aligned}&H_0: \beta_{1} =  \beta_{2} = \dots = \beta_p = 0 \\ 
&H_a: \text{at least one }\beta_j \text{ is not equal to 0}\end{aligned}$$
]

<br>

- The statistic for this test is the $F$ test statistic in the ANOVA table 

- We calculate the p-value using an $F$ distribution with $p$ and $(n-p-1)$ degrees of freedom

---

## ANOVA F Test in R

```{r}
model0 <- lm(Tip ~ 1, data=tips)
```

--

```{r}
model1 <- lm(Tip ~ Party + Meal + Age , data = tips)
```

--

```{r}
kable(anova(model0,model1),format="html")
```

---

### Testing subset of coefficients

- Sometimes we want to test whether a subset of coefficients are all equal to 0

- This is often the case when we want test 
    - whether a categorical variable with $k$ levels is a significant predictor of the response
    - whether the interaction between a categorical and quantitative variable is significant

- To do so, we will use the  <font class="vocab3">Nested F Test </font> 

---

## Nested F Test

- Suppose we have a full and reduced model: 

$$\begin{aligned}&\text{Full}: y = \beta_0 + \beta_1 x_1 + \dots + \beta_q x_q + \beta_{q+1} x_{q+1} + \dots \beta_p x_p \\
&\text{Red}: y = \beta_0 + \beta_1 x_1 + \dots + \beta_q x_q\end{aligned}$$
<br>

- We want to test whether any of the variables $x_{q+1}, x_{q+2}, \ldots, x_p$ are significant predictors. To do so, we will test the hypothesis: 

.alert[
$$\begin{aligned}&H_0: \beta_{q+1} =  \beta_{q+2} = \dots = \beta_p = 0 \\ 
&H_a: \text{at least one }\beta_j \text{ is not equal to 0}\end{aligned}$$
]

---

## Nested F Test

- The test statistic for this test is 


$$F = \frac{(RSS_{reduced} - RSS_{full})\big/(p_{full} - p_{reduced})}{RSS_{full}\big/(n-p_{full}-1)}$$
<br> 

- Calculate the p-value using the F distribution with $(p_{full} - p_{reduced})$ and $(n-p_{full}-1)$ degrees of freedom

---

### Is `Meal` a significant predictor of tips?

```{r echo=F}
model.tips <- lm(Tip ~ Party + Age + Meal,data=tips)
kable(tidy(model.tips),format="html", digits=3)
```

---

### Tips data:  Nested F Test

$$\begin{aligned}&H_0: \beta_{late night} = \beta_{lunch} = 0\\
&H_a: \text{ at least one }\beta_j \text{ is not equal to 0}\end{aligned}$$

--

```{r}
reduced <- lm(Tip ~ Party + Age, data = tips)
```

--

```{r}
full <- lm(Tip ~ Party + Age + Meal, data = tips)
```

--

```{r}
kable(anova(full,reduced),format="html")
```

--

**At least one coefficient associated with `Meal` is not zero. Therefore, `Meal` is a significant predictor of `Tips`.**

---

class: middle

.question[
Why is it not good practice to use the individual p-values to determine whether `Meal` (or any categorical variable with $k > 2$ levels) is significant?
]

---

## Practice with Interactions 

```{r echo=F}
full <- lm(Tip ~ Party + Age + Meal + Meal*Party, data = tips)
kable(tidy(full),format="html")
```

.question[
1. Write the general form of the model. 
2. Write the model for `Meal == "Late Night"`. 
3. How does the mean change when `Meal == "Late Night"`? 
4. How does the slope of `Party` change when `Meal == "Late Night"`?
]
---

### Nested F test for interactions

**Is the interaction between `Party` and `Meal` significant?**

```{r}
reduced <- lm(Tip ~ Party + Age + Meal, data = tips)
```

--

```{r}
full <- lm(Tip ~ Party + Age + Meal + Meal*Party, data = tips)
```

--

```{r}
kable(anova(full,reduced),format="html")
```

---

class: middle, center

## Influential and Leverage Points

---

## Influential Observations

An observation is <font class="vocab3">influential</font> if removing it substantially changes the coefficients of the regression model 

```{r,echo=F}
library(cowplot)
set.seed(12)
n <- 20
x <- c(runif(n,0,1))
y <- 3*x + rnorm(n,0,.5)
new.pt <- data.frame(x=2,y=0)
x.new <- c(x,2)
y.new <- c(y,0)
data <- bind_cols(x=x.new,y=y.new)
p1<- ggplot(data=data,aes(x=x,y=y))+geom_point(alpha =0.5)  + 
              geom_point(data=new.pt,color="red",size=3,shape=17) + 
  geom_smooth(method="lm",se=F) + 
  labs(title = "With Influential Point")+ theme_light()+
  theme(title=element_text(hjust=0.5,size=14)) + 
  scale_x_continuous(limits = c(0,2)) 

data2 <- bind_cols(x=x,y=y)
p2 <- ggplot(data=data2,aes(x=x,y=y))+geom_point(alpha=0.5) + geom_smooth(method="lm",se=F) + 
  labs(title="Without Influential Point") + 
  scale_x_continuous(limits = c(0, 2)) + theme_light() + theme(title=element_text(hjust=0.5,size=14))  
plot_grid(p1,p2,ncol=2)
```

---

## Influential Observations 

- In addition to the coefficients, influential observations can have a large impact on the standard errors

- Occasionally these observations can be identified in the scatterplot
  + This is often not the case - especially when dealing with multivariate data

- We will use measures to quantify an individual observation's influence on the regression model 
  + **Cook's distance**, **leverage**, **standardized residuals**

---

## Leverage

- <font class="vocab3">Leverage: </font> measure of the distance between an observation's explanatory variable values and the average explanatory variables for the whole data set
  
- An observation has large leverage if its combination of values for the explanatory variables is very far from the typical combinations in the data 
  + It is **<u>potentially</u>** an influential point, i.e. may have a large impact on the coefficient estimates and standard errors


- **Note:** Identifying points with large leverage has nothing to do with the values of the response variables

---

## Caculating Leverage

- <font class="vocab">Simple Regression:</font> leverage of the $i^{th}$ observation is 
$$h_i = \frac{(X_i - \bar{X})^2}{\sum_{j=1}^{n}(X_j-\bar{X})^2} + \frac{1}{n}$$


- <font class="vocab">Multiple Regression:</font> leverage of the $i^{th}$ observation is the $i^{th}$ diagonal of
$$\mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T$$

---

## Large Leverage

- Values of leverage are between $\frac{1}{n}$ and 1 for each observation 

- The average leverage for all observations in the data set is $\frac{p}{n}$

- Determination of "large leverage" is relatively arbitrary 
  + one threshold is $\frac{2p}{n}$

- Observations with large leverage tend to have small residuals

---

## Large Leverage

- Questions to check if you identify points with large leverage: 
  + Are they a result of data entry errors?
  + Are they in the scope for the individuals for which you want to make predictions?
  + Are they impacting the estimates of the model coefficients, especially for interactions?

- Just because a point has large leverage does not necessarily mean it will have a substantial impact on the regression. Therefore you should check other measures.

---

### Standardized (Studentized) Residuals

- What is the best way to identify outliers (points that don't fit the pattern from the regression line)? 
  
--

- Look for points that have large residuals

--

- We want a common scale, so we can more easily identify "large" residuals

--

- We will look at each residual divided by its standard error

---

## Standardized Residuals 

$$\color{blue}{standres_i = \frac{e_i}{\hat{\sigma}\sqrt{1-h_i}}}$$
<br>

--

- The standard error of a residual, $\hat{\sigma}\sqrt{1-h_i}$ depends on the value of the explanatory variables 

--


- Residuals for observations that are high leverage have smaller variance than residuals for observations that are low leverage 

  + This is because the regression line tries to fit high leverage observations as closely as possible

--

- Standardized residuals follow a $N(0,1)$ distribution

---

## Standardized Residuals 

- Values with very large standardized residuals are outliers, since they don't fit the pattern determined by the regression model 

- Since the standardized residuals follow $N(0,1)$ distribution, we expect about 5% of the standardized residuals to have magnitude greater than 2

- Observations with large standardized residuals are outliers but may not have an impact on the regression line

- <font class="vocab">Good Practice: </font>Make residual plots with standardized residuals, since they better reflect constant variance when that assumption holds

---

### What to do with outliers?

- It is **<font color="green">OK</font>** to drop an observation based on the **<u>explanatory variables</u>** if...
  + It is meaningful to drop the observation given the context of the problem 
  + You intended to build a model on a smaller range of the explanatory variables. Mention this in the write up of the results and be careful to avoid extrapolation when making predictions

- It is **<font color="red">not OK</font>** to drop an observation based on the response variable
  + These are legitimate observations and should be in the model
- You can try transformations, increasing the sample size by collecting more data, or doing nothing. 
  + It is fine to have some outliers in the data

---

### Motivating Cook's Distance

- If a observation has a large impact on the estimated regression coefficients, when we drop that observation...
  + The estimated coefficients should change 
  + The predicted $Y$ value for that observation should change
- One way to determine each observation's impact could be to delete it, rerun the regression, compare the predicted $Y$ values from the new and original models
  + This could be very time consuming 
- Instead, we can use <font class="vocab3">Cook's Distance</font> which gives a measure of the change in the predicted $Y$ value when an observation is dropped

---

## Cook's Distance

- <font class="vocab3">Cook's Distance: </font> Measure of an observation's overall impact, i.e. the effect removing the observation has on the estimated coefficients

- For the $i^{th}$ observation, we can calculate Cook's Distance as 
$$\color{blue}{D_i = \frac{1}{p}(standres_i)^2\bigg(\frac{h_i}{1-h_i}\bigg)}$$

---

## Large Cook's Distance

- It is **<font color="green">OK</font>** to drop an observation based on the **<u>explanatory variables</u>** if...
  + It is meaningful to drop the observation given the context of the problem 
  + You intended to build a model on a smaller range of the explanatory variables. Mention this in the write up of the results and be careful to avoid extrapolation when making predictions
- It is **<font color="red">not OK</font>** to drop an observation based on the response variable
  + These are legitimate observations and should be in the model
- You can try transformations or increasing the sample size by collecting more data

---

## Using these measures

- standardized residuals, leverage, and Cook's Distance should all be examined together 

- Examine plots of the measures to identify observations that may have an impact on your regression model 

- Thresholds for flagging potentially influential observations used by some software: 
  + $D_i > 1$
  + $h_i > 2p/n$
  + $|standres_i| > 2$
  
---

## Example: Diamonds Data 

- We will build a regression model using 300 randomly selected observations for the `diamonds` data 

```{r}
set.seed(12)
diamonds <- diamonds %>% filter(carat < 1.1)
diamonds_data <- diamonds %>% 
  sample_n(300) %>%
  mutate(logprice= log(price),
         caratCent = carat - mean(carat),
         caratCent.sq = caratCent^2, 
         carat.sq = carat^2)
```

```{r}
diamonds_model <- lm(logprice ~ caratCent + caratCent.sq + 
                       color + clarity, data=diamonds_data)
kable(tidy(diamonds_model),format="html")
```

---

##Example: Diamonds

- We can output the model diagnostics (along with the predicted values and residuals) using the <font class="vocab">`augment`</font> function in the broom package

```{r}
#add leverage, cook's distance and standardized residuals to data set
diamonds_output <- augment(diamonds_model) %>%
  mutate(obs_num = row_number())
```

```{r}
glimpse(diamonds_output)
```
---

## Diamonds: Leverage

```{r}
ggplot(data=diamonds_output, aes(x=obs_num,y=.hat)) + 
  geom_point(alpha=0.5) + 
  geom_hline(yintercept=0.1,color="red")+
  labs(x="Observation Number",y="Leverage",title="Leverage")
```


---

### Diamonds: Cook's Distance

```{r}
ggplot(data=diamonds_output, aes(x=obs_num,y=.cooksd)) + 
  geom_point() + 
  geom_hline(yintercept=1,color="red")+
  labs(x="Observation Number",y="Cook's Distance",title="Cook's Distance")
```

---

### Diamonds: Standardized Residuals

```{r echo=F}
p1 <- ggplot(data=diamonds_output, aes(x=.fitted,y=.std.resid)) +
  geom_point() + 
  geom_hline(yintercept=0,color="red")
p2 <- ggplot(data=diamonds_output, aes(x=caratCent,y=.std.resid)) + 
  geom_point() + 
  geom_hline(yintercept=0,color="red")
p3 <- ggplot(data=diamonds_output, aes(x=color,y=.std.resid)) + 
  geom_boxplot() + 
  geom_hline(yintercept=0,color="red")
p4 <- ggplot(data=diamonds_output, aes(x=clarity,y=.std.resid)) + 
  geom_boxplot() + 
  geom_hline(yintercept=0,color="red")
plot_grid(p1,p2,p3,p4,ncol=2)
```

---

## Observations of Interest

```{r}
diamonds_output %>% filter(.hat > 0.2) %>% 
  select(logprice,color,clarity,caratCent)
```
---

class: middle, center

## Multicollinearity

---

### Why multicollinearity is a problem

- We can't include two variables that have a perfect linear association with each other

- If we did so, we could not pick a unique best fit model


---

### Why multicollinearity is a problem

- Ex. Suppose the true population regression equation is $\mu\{Y|X\} = 3 + 4X$

--

- Suppose we try estimating that regression model using the variables $X$ and $Z = X/10$
$$\begin{aligned}\hat{\mu}\{Y|X\} &= \hat{\beta}_0 + \hat{\beta}_1X  + \hat{\beta}_2\frac{X}{10}\\
&= \hat{\beta}_0 + \bigg(\hat{\beta}_1 + \frac{\hat{\beta}_2}{10}\bigg)X\end{aligned}$$

--

- We can set $\hat{\beta}_1$ and $\hat{\beta}_2$ to any two numbers such that $\hat{\beta}_1 + \frac{\hat{\beta}_2}{10} = 4$
  + The data then is unable to choose the "best" combination of $\hat{\beta}_1$ and $\hat{\beta}_2$
  
---

### Why multicollinearity is a problem

- When we have almost perfect collinearities (i.e. highly correlated explanatory variables), the standard errors for our regression coefficients inflate

- In other words, we lose precision in our estimates of the regression coefficients 
  
---
 
## Detecting Multicollinearity

Multicollinearity may occur when...
- There are very high correlations $(r > 0.9)$ among two or more explanatory variables, especially for smaller sample sizes

--

- One (or more) explanatory variables is an almost perfect linear combination of the others 

--

- Include quadratic terms without first mean-centering the variables before squaring

--

- Including interactions with two or more continuous variables

---

 

## Detecting Multicollinearity 

- Look at a correlation matrix of the explanatory variables, including all dummy variables 
  + Look out for values close to 1 or -1

- If you think one explanatory variable is an almost perfect linear combination of other explanatory variables, you can run a regression of that explanatory variable vs. the others and see if $R^2$ is close to 1

---

## Detetcing Multicollinearity (VIF)

- <font class="vocab">Variance Inflation Factor (VIF)</font>: Measure of multicollinearity 

$$\color{blue}{VIF = \frac{1}{1-R^2_X}}$$

where $R^2_X$ is the proportion of variation $X$ that is explained by the linear combination of the other explanatory variables in the model.


- Typically $VIF > 10$ indicates concerning multicollinearity


- Use the <font class="vocab">`vif()`</font> function in the `car` package to calculate VIF

---

## Diamonds VIF

- Calculate VIF using the <font class="vocab">`vif`</font> function in the rms package 

```{r}
library(rms)
tidy(vif(diamonds_model))
```


<!-- --- -->

<!-- ## Diamonds VIF -->

<!-- ```{r} -->
<!-- diamonds_data %>% group_by(clarity) %>% summarise(n=n()) -->
<!-- ``` -->







