---
title: "Multiple Linear Regression"
author: "Dr. Maria Tackett"
date: "02.04.19"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "sta210-slides.css"
    logo: img/sta210-sticker-icon.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE, echo=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
	fig.align = "center",
	fig.height =5,
	fig.width = 8,
	message = FALSE,
	warning = FALSE
)
```



## Announcements

- Reading 04 due Wed, Feb 13

- Lab 02 due Wed, Feb 13 

- HW 02 due Mon, Feb 11 (posted after class)


---

 
## R Packages used in the notes

```{r}
library(tidyverse)
library(knitr)
library(broom)
library(Sleuth3) # case 1202 dataset
library(cowplot) # use plot_grid function
```

---

## Motivating Example

- In the 1970s Harris Trust and Savings Bank was sued for discrimination on the basis of sex. 

- The defense presented an analysis of the salaries for skilled, entry-level clerical employees as evidence. 

- **Question**: Did female employees receive lower starting salaries on average than male employees with similar experience and qualifications?

---

 

## Data

```{r,echo=F}
library("readr")
wages <- case1202 %>% 
  mutate(Female = as.factor(ifelse(Sex=="Female",1,0))) %>%
  select(-Sal77,-Sex)
```

```{r}
glimpse(wages)
```

---

## Variables

**Explanatory**
- <font class="vocab">`Educ`: </font>years of education
- <font class="vocab">`Exper`: </font>months of previous work experience (before hire at bank)
- <font class="vocab">`Female`: </font>1 if female, 0 if male
- <font class="vocab">`Senior`: </font>months worked at bank since hire
- <font class="vocab">`Age`: </font>age in months

**Response**
- <font class="vocab">`Bsal`: </font>annual salary at time of hire

---

## Salary comparison

- <font class="vocab">Question: </font> Did female employees receive lower starting salaries on average than male employees with similar experience and qualifications?

```{r}
ggplot(data=wages,aes(x=Bsal,fill=Female)) + 
  geom_density(alpha=0.5) + 
  labs(y="Starting Salary", title ="Starting Salary",
       subtitle = "by Gender") 
```

---

### Using ANOVA

$$\begin{aligned}&H_0: \mu_F = \mu_M\\
&H_a: \mu_F \neq \mu_M\end{aligned}$$

```{r,echo=F}
anova <- aov(Bsal ~ Female, data = wages)
kable(tidy(anova),format="markdown",digits=3)
```

.question[
- What's your conclusion?

- What is a disadvantage to using this method to answer the question?
]

---

## Salary vs. Other Variables

```{r}
pairs(Bsal ~ Senior + Age + Educ + Exper, data=wages)
```

---

## Multiple Regression Model

- We will calculate a multiple linear regression model with the following form: 
<br> 

$$\color{blue}{Bsal = \\ 
\beta_0 + \beta_1 \text{Senior} + \beta_2 \text{Age} + \beta_3 \text{Educ} + \beta_4 \text{Exper} + \beta_5 \text{Female}}$$
<br> 


- Similar to simple linear regression, this model assumes that at each combination of the predictor variables, the values `Bsal` follow a Normal distribution

---

## Regression Model

- Recall: The simple linear regression model assumes 

$$y|x\sim N(\beta_0 + \beta_1 x, \sigma^2)$$
--

- Similarly: The multiple linear regression model assumes 

$$\color{blue}{y|x_1, x_2, \ldots, x_p \sim N(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p, \sigma^2)}$$
--

- For a given observation $(x_{i1}, x_{i2} \ldots, x_{iP}, y_i)$, we can rewrite the previous statement as 

$$\color{blue}{y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip} + \epsilon_{i} \hspace{5mm} \epsilon_i \sim N(0,\sigma^2)}$$

---

## Regression Model 

- At any combination of $x's$, the true mean value of $y$ is
$$\color{blue}{y = \beta_0 + \beta_1 x_{1} + \beta_2 x_2 + \dots + \beta_p x_p}$$
--

- We will use multiple linear regression to calculate an estimate of the model 
$$\color{blue}{\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_{1} + \hat{\beta}_2 x_2 + \dots + \hat{\beta}_p x_{p}}$$

---

## Regression Output

```{r}
model <- lm(Bsal ~ Senior + Age + Educ + Exper + Female, data=wages)
kable(tidy(model),format="html",digits=3)
```
---

## Interpreting $\hat{\beta}_j$

- An estimated coefficient $\hat{\beta}_j$ is the amount $y$ is expected to change when $x_j$ increases by one unit **holding the values all other explanatory variables constant**

--

- *Example:* The estimated coefficient for `Educ` is 92.31. This means for each additional year of education an employee has, we expect starting salary to increase by about $92.31, holding all over variables constant.

---

### Hypothesis Tests for $\hat{\beta}_j$

- We want to test whether a particular coefficient has a value of 0 in the population, given all other variables in the model: 

$$\begin{aligned}&H_0: \beta_j = 0 \\ &H_a: \beta_j \neq 0\end{aligned}$$

- The test statistic reported in R is the following: 

$$\color{blue}{\text{test statistic} = \frac{\hat{\beta}_j - 0}{SE(\hat{\beta}_j)}}$$

---

### Hypothesis Tests for $\hat{\beta}_j$

- Suppose we want to test the following hypothesis 
$$H_0: \beta_j = \beta_{j0}$$

- The test statistic will take the usual form: 

$$\color{blue}{\text{test statistic} = \frac{\hat{\beta}_j - \beta_{j0}}{SE(\hat{\beta}_j)}}$$


- We calculate the **p-value:** using a $t$ distribution with $(n - p - 1)$ degrees of freedom 
    - Note: There are $(p+1)$ total terms in the model 
  
---

## Salary 

```{r,echo=F}
kable(tidy(model),format="html",digits=3)
```

.question[
Given the other variables in the model, are the following significant predictors of `Bsal`? 

- Education (`educ`)
- Experience (`exper`)
]

---

## Salary

```{r,echo=F}
kable(tidy(model),format="html",digits=3)
```

.question[
Given the other variables in the model, which variable has a stronger linear relationship with `Bsal`? 

`Senior` or `Educ`?
]

---

### Confidence Interval for $\beta_j$

- We calculate the confidence interval in the usual way 

$$\text{estimate} \pm t^* \times SE$$
--

- Therefore, the confidence interval for $\beta_j$ is 
$$\color{blue}{\hat{\beta}_j \pm t^* SE(\hat{\beta}_j)}$$

- We calculate $t^*$ using a $t$ distribution with $(n - p - 1)$ degrees of freedom

---

### CI for `Educ`

```{r,echo=F}
kable(tidy(model,conf.int=TRUE),format="html",digits=3)
```

---

### Notes about CI and Hypothesis Tests

- If the sample size is large enough, the test will probably reject $H_0: \beta_j=0$
  + Consider the <font class="vocab">practical significance</font> of the result not just the statistical significance 
 
  
--

- If the sample size is small, there may not be enough evidence to reject $H_0: \beta_j=0$
    - When you fail to reject the null hypothesis, **DON'T** immediately conclude that the variable has no association with the response. 
    - There may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association.
    

---

## Prediction

- We calculate predictions the same as with simple linear regression 

- **Example:** Suppose we want to predict the starting wages for a female who is 28 years old with 12 years of education, 11 months seniority and 2 years of prior experience. 

$$\begin{aligned}\hat{bsal} = & 6277.893 - 22.582 \times \text{Senior} + 0.631 \times \text{Age} \\ &+ 92.306 \times \text{Educ} + 0.501 \times \text{Exper} - 767.913 \times \text{Female}\end{aligned}$$ 

--

```{r}
6277.893 - 22.582 * 11 + 0.631 * 28 + 92.306 * 12 + 0.501 * 24 - 767.913 * 1
```

---

## Prediction

- Just like with simple linear regression, we can use the <font class="vocab">`predict.lm()`</font> function in R to calculate the appropriate intervals for our predicted values 


--

- Suppose we want to predict the starting wages for a female who is 28 years old with 12 years of education, 11 months seniority and 2 years of prior experience. 

--

```{r}
x0 = data.frame(Senior= 11, Age = 28, Educ = 12, Exper = 24, Female = "1")
predict.lm(model,x0,interval="prediction")
```

---

## Prediction

Suppose we want to predict the mean age for the subset of all females who are 28 years old with 12 years of education, 11 months of seniority and 2 years of prior experience. 

.question[
- How will the predicted value change?

- How will the interval change? 
]

--
```{r}
x0 = data.frame(Senior= 11, Age = 28, Educ = 12, Exper = 24, Female = "1")
predict.lm(model,x0,interval="confidence") #<<
```

---

## Cautions

- <font class="vocab3">Do not extrapolate!</font> Because there are multiple explanatory variables, you can extrapolation in many ways

--

- The multiple regression model only shows <font class="vocab3"> association, not causality </font>
  + To prove causality, you must have a carefully designed experiment or carefully account for confounding variables in an observational study 

---

class: middle, center

## Assumptions 

---

## Assumptions 

The confidence intervals and hypothesis tests are reliable only when the regression assumptions are reasonably satisfied

1. <font class="vocab">Linearity: </font> Response variable has a linear relationship with the explanatory variables in the model

2. <font class="vocab">Constant Variance: </font>The regression variance is the same for all set of predictor variables $(x_1, \ldots, x_p)$

3. <font class="vocab">Normality: </font> For a given $(x_1, \ldots, x_p)$, the distribution of $y$ around its mean is Normal

4. <font class="vocab">Independence: </font>All observations are independent

---

## Scatterplots 

- Look at a scatterplot of the response variable vs. each of the predictor variables in the exploratory data analysis before calculating the regression model 

- This is a good way to check for obvious departures from linearity 
    - Could be an indication that a higher order term or transformation is needed (will discuss this next class)

---

## Residual Plots

- **Plot the residuals vs. the predicted values**
    - Can expose issues such at outliers or nonconstant variance 

- **Plot the residuals vs. each of the predictors**
    - Can expose issues between the response and a predictor variable that didn't show in the exploratory data analysis
    - Use boxplots to plot residuals versus categorical predictor variables

--

- Residual plots should show no systematic pattern

- Plot a histogram and QQ-plot of the residuals to check Normality 

---

### Scatterplots

```{r}
pairs(Bsal ~ Senior + Age + Educ + Exper, data=wages)
```

---

### Residuals vs. Predicted Values

```{r}
wages <- wages %>% 
  mutate(predicted = predict.lm(model), residuals = resid(model))

ggplot(data=wages,aes(x=predicted, y=residuals)) + 
  geom_point() + 
  geom_hline(yintercept=0,color="red") +
  labs(title="Residuals vs. Predicted Values") 
```

---

### Residuals vs. Predictors

```{r,echo=F}
p1 <- ggplot(data=wages,aes(x=Senior,y=residuals)) + 
  geom_point() + 
  geom_hline(yintercept=0,color="red") +
  labs(x = "Seniority", y="Residuals")

p2 <- ggplot(data=wages,aes(x=Age,y=residuals)) + 
  geom_point() + 
  geom_hline(yintercept=0,color="red") +
  labs(x = "Age", y="Residuals")

p3 <- ggplot(data=wages,aes(x=Educ,y=residuals)) + 
  geom_point() + 
  geom_hline(yintercept=0,color="red") +
  labs(x = "Education", y="Residuals")

p4 <- ggplot(data=wages,aes(x=Exper,y=residuals)) + 
  geom_point() + 
  geom_hline(yintercept=0,color="red") +
  labs(x = "Experience", y="Residuals")

plot_grid(p1,p2,p3,p4,nrow=2)
```

---

### Residuals vs. Predictors 

```{r}
ggplot(data=wages,aes(x=Female,y=residuals)) + 
  geom_boxplot() + 
  geom_hline(yintercept=0,color="red") +
  labs(x = "Female", 
       y="Residuals")
```

---

### Normality of Residuals

.pull-left[
```{r,echo=F}
ggplot(data=wages,aes(x=residuals)) + geom_histogram(fill="steelblue2",color="black") + 
  labs(title="Distribution of Residuals") + 
    theme(plot.title=element_text(hjust=0.5,size=16))
```
]

.pull-right[
```{r,echo=F}
ggplot(data=wages,aes(sample=residuals)) + stat_qq() + stat_qq_line() +
  labs(title="QQ-Plot of Residuals") + 
    theme(plot.title=element_text(hjust=0.5,size=16))
```
]

---

class: middle, center

## Math  Foundation

---

## Regression Model 

- The multiple linear regression model assumes 

$$\color{blue}{y|x_1, \ldots, x_p \sim N(\beta_0 + \beta_1 x_1 + \dots + \beta_p x_p, \sigma^2)}$$
<br> 

--

- For a given observation $(x_{i1}, \ldots, x_{ip}, y_i)$, we can rewrite the previous statement as 

$$\color{blue}{Y_i = \beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip} + \epsilon_{i} \hspace{10mm} \epsilon_i \sim N(0,\sigma^2)}$$

---

## Estimating $\sigma^2$

- For a given observation $(x_{i1},\ldots,x_{ip}, y_i)$ the residual is 
$$e_i = y_{i} - (\hat{\beta}_0 + \hat{\beta}_1 x_{i1} + \dots + \hat{\beta}_p x_{ip})$$

--

- The <font class="vocab">estimated regression variance</font> is

.alert[
$$\hat{\sigma}^2= \frac{RSS}{n-p-1} = \frac{\sum\limits_{i=1}^n[y_{i} - (\hat{\beta}_0 + \hat{\beta}_1 x_{i1} + \dots + \hat{\beta}_p x_{ip})]^2}{n-p-1}$$

---

## Calculating $\hat{\sigma}^2$

Salary: Estimating $\hat{\sigma}^2$

```{r}
sigma(model)^2
```


--

```{r}
kable(tidy(aov(model)),format="html",digits=3)
```

---

## Before next class

- [Reading 04](https://www2.stat.duke.edu/courses/Spring19/sta210.001/reading/reading-04.html)